{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../\")\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'osgeo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3cbec6ae97ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mosgeo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mogr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgdal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0muuid\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0muuid4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'osgeo'"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, Union, Optional, Tuple, List\n",
    "from osgeo import ogr, gdal\n",
    "from uuid import uuid4\n",
    "import rtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buteo.project_types import Number\n",
    "from buteo.raster.io import (\n",
    "    open_raster,\n",
    "    to_raster_list,\n",
    "    raster_to_array,\n",
    "    internal_raster_to_disk,\n",
    "    internal_raster_to_metadata,\n",
    "    array_to_raster,\n",
    "    rasters_are_aligned,\n",
    ")\n",
    "from buteo.vector.io import (\n",
    "    internal_vector_to_metadata,\n",
    "    internal_vector_to_disk,\n",
    "    open_vector,\n",
    ")\n",
    "from buteo.vector.reproject import internal_reproject_vector\n",
    "from buteo.raster.clip import clip_raster, internal_clip_raster\n",
    "from buteo.raster.resample import internal_resample_raster\n",
    "from buteo.utils import overwrite_required, remove_if_overwrite, progress, type_check\n",
    "from buteo.gdal_utils import ogr_bbox_intersects\n",
    "from buteo.machine_learning.ml_utils import Mish, mish, load_mish, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstitute_raster(\n",
    "    blocks: np.ndarray,\n",
    "    raster_height: int,\n",
    "    raster_width: int,\n",
    "    size: int,\n",
    "    offset: Tuple[Number, Number],\n",
    "    border_patches: bool,\n",
    "    border_patches_x: bool,\n",
    "    border_patches_y: bool,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Recombines blocks into an array.\n",
    "    Args:\n",
    "        blocks (ndarray): A numpy array with the values to recombine. The shape\n",
    "        should be (blocks, rows, column, channel).\n",
    "        raster_height (int): height in pixels of target raster.\n",
    "        raster_width (int): width in pixels of target raster.\n",
    "        size (int): size of patches in pixels. (square patches.)\n",
    "        offset (tuple): A tuple with the offset of the blocks (x, y)\n",
    "        border_patches (bool): Does the patches contain border_patches?\n",
    "        border_patches_x (bool): Does the patches contain border_patches on the x axis?\n",
    "        border_patches_y (bool): Does the patches contain border_patches on the y axis?\n",
    "    Returns:\n",
    "        A reconstituted raster.\n",
    "    \"\"\"\n",
    "    type_check(blocks, [np.ndarray], \"blocks\")\n",
    "    type_check(raster_height, [int], \"raster_height\")\n",
    "    type_check(raster_width, [int], \"raster_width\")\n",
    "    type_check(size, [int], \"size\")\n",
    "    type_check(offset, [tuple], \"offset\")\n",
    "    type_check(border_patches, [bool], \"border_patches\")\n",
    "    type_check(border_patches_x, [bool], \"border_patches_x\")\n",
    "    type_check(border_patches_y, [bool], \"border_patches_y\")\n",
    "    ref_shape = [raster_height - offset[1], raster_width - offset[0]]\n",
    "    if offset != (0, 0):\n",
    "        border_patches = False\n",
    "        border_patches_x = False\n",
    "        border_patches_y = False\n",
    "    if border_patches and (border_patches_x or border_patches_y):\n",
    "        if border_patches_x:\n",
    "            ref_shape[1] = ((ref_shape[1] // size) * size) + size\n",
    "        if border_patches_y:\n",
    "            ref_shape[0] = ((ref_shape[0] // size) * size) + size\n",
    "    reshape = blocks.reshape(\n",
    "        ref_shape[0] // size,\n",
    "        ref_shape[1] // size,\n",
    "        size,\n",
    "        size,\n",
    "        blocks.shape[3],\n",
    "        blocks.shape[3],\n",
    "    )\n",
    "    swap = reshape.swapaxes(1, 2)\n",
    "    destination = swap.reshape(\n",
    "        (ref_shape[0] // size) * size, (ref_shape[1] // size) * size, blocks.shape[3]\n",
    "    )\n",
    "\n",
    "    # Order: Y, X, Z\n",
    "    if border_patches and (border_patches_x or border_patches_y):\n",
    "        x_offset = 0\n",
    "        y_offset = 0\n",
    "        if border_patches_x:\n",
    "            x_offset = int(ref_shape[1] - raster_width)\n",
    "            x_edge = destination[:raster_height, -(size - x_offset) :, :]\n",
    "            destination[:raster_height, -size:-x_offset, :] = x_edge\n",
    "        if border_patches_y:\n",
    "            y_offset = int(ref_shape[0] - raster_height)\n",
    "            y_edge = destination[-(size - y_offset) :, :raster_width, :]\n",
    "            destination[-size:-y_offset, :raster_width, :] = y_edge\n",
    "        if border_patches_y and border_patches_y:\n",
    "            corner = destination[-(size - y_offset) :, -(size - x_offset) :, :]\n",
    "            destination[-size:-y_offset, -size:-x_offset, :] = corner\n",
    "        destination = destination[:raster_height, :raster_width, 0 : blocks.shape[3]]\n",
    "    return destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blocks_to_raster(\n",
    "    blocks: np.ndarray,\n",
    "    reference: Union[str, gdal.Dataset],\n",
    "    out_path: Union[str, None] = None,\n",
    "    offsets: Union[list, tuple, np.ndarray] = [],\n",
    "    border_patches: bool = True,\n",
    "    generate_zero_offset: bool = True,\n",
    "    merge_method: str = \"median\",\n",
    "    output_array: bool = False,\n",
    "    dtype=None,\n",
    "    verbose: int = 1,\n",
    ") -> Union[str, np.ndarray]:\n",
    "    \"\"\"Recombines a series of blocks to a raster. OBS: Does not work if the patch\n",
    "        extraction was done with clip geom.\n",
    "    Args:\n",
    "        blocks (ndarray): A numpy array with the values to recombine. The shape\n",
    "        should be (blocks, rows, column, channel).\n",
    "        reference (str, raster): A reference raster to help coax the blocks back\n",
    "        into shape.\n",
    "    **kwargs:\n",
    "        out_path (str | None): Where to save the reconstituted raster. If None\n",
    "        are memory raster is returned.\n",
    "        offsets (tuple, list, ndarray): The offsets used in the original. A (0 ,0)\n",
    "        offset is assumed.\n",
    "        border_patches (bool): Do the blocks contain border patches?\n",
    "        generate_zero_offset (bool): if True, an offset is inserted at (0, 0)\n",
    "        if none is present.\n",
    "        merge_method (str): How to handle overlapping pixels. Options are:\n",
    "        median, average, mode, min, max\n",
    "        output_array (bool): If True the output will be a numpy array instead of a\n",
    "        raster.\n",
    "        verbose (int): If 1 will output messages on progress.\n",
    "    Returns:\n",
    "        A reconstituted raster.\n",
    "    \"\"\"\n",
    "    type_check(blocks, [str, np.ndarray], \"blocks\")\n",
    "    type_check(reference, [str, gdal.Dataset], \"reference\")\n",
    "    type_check(out_path, [str], \"out_path\", allow_none=True)\n",
    "    type_check(offsets, [list, tuple, np.ndarray], \"offsets\", allow_none=True)\n",
    "    type_check(border_patches, [bool], \"border_patches\")\n",
    "    type_check(generate_zero_offset, [bool], \"bool\")\n",
    "    type_check(merge_method, [str], \"merge_method\")\n",
    "    type_check(output_array, [bool], \"output_array\")\n",
    "    type_check(verbose, [int], \"verbose\")\n",
    "    if isinstance(blocks, str):\n",
    "        try:\n",
    "            blocks = np.load(blocks)\n",
    "        except:\n",
    "            raise ValueError(f\"Failed to parse blocks: {blocks}\")\n",
    "    if verbose == 1:\n",
    "        print(\"Reconstituting blocks into target raster.\")\n",
    "    metadata = internal_raster_to_metadata(reference)\n",
    "    border_patches_x = False\n",
    "    border_patches_y = False\n",
    "    if blocks.shape[1] != blocks.shape[2]:\n",
    "        raise ValueError(\n",
    "            \"The input blocks must be square. Rectangles might supported in the future.\"\n",
    "        )\n",
    "    size = blocks.shape[1]\n",
    "    if metadata[\"width\"] % size != 0 and border_patches:\n",
    "        border_patches_x = True\n",
    "    if metadata[\"height\"] % size != 0 and border_patches:\n",
    "        border_patches_y = True\n",
    "\n",
    "    # internal offset array. Avoid manipulating the og array.\n",
    "    in_offsets = []\n",
    "    if generate_zero_offset:\n",
    "        if offsets is not None:\n",
    "            if (0, 0) not in offsets:\n",
    "                in_offsets.append((0, 0))\n",
    "        else:\n",
    "            in_offsets.append((0, 0))\n",
    "    for offset in offsets:\n",
    "        if offset != (0, 0):\n",
    "            if not isinstance(offset, (list, tuple)) or len(offset) != 2:\n",
    "                raise ValueError(\n",
    "                    f\"offset must be a list or tuple of two integers. Recieved: {offset}\"\n",
    "                )\n",
    "            in_offsets.append((offset[0], offset[1]))\n",
    "        elif not generate_zero_offset:\n",
    "            in_offsets.append((offset[0], offset[1]))\n",
    "\n",
    "    # Easier to read this way.\n",
    "    has_offsets = False\n",
    "    if generate_zero_offset and len(in_offsets) > 1:\n",
    "        has_offsets = True\n",
    "    if not generate_zero_offset and len(in_offsets) > 0:\n",
    "        has_offsets = True\n",
    "    if has_offsets:\n",
    "        passes = []\n",
    "        previous = 0\n",
    "        largest_x = 0\n",
    "        largest_y = 0\n",
    "        for index, offset in enumerate(in_offsets):\n",
    "            passes.append(\n",
    "                np.ma.masked_all(\n",
    "                    (\n",
    "                        metadata[\"height\"],\n",
    "                        metadata[\"width\"],\n",
    "                        blocks.shape[3],\n",
    "                    ),\n",
    "                    dtype=metadata[\"datatype\"] if dtype is None else dtype,\n",
    "                ),\n",
    "            )\n",
    "            if index == 0:\n",
    "                x_blocks = ((metadata[\"width\"] - offset[0]) // size) + border_patches_x\n",
    "                y_blocks = ((metadata[\"height\"] - offset[1]) // size) + border_patches_x\n",
    "            else:\n",
    "                x_blocks = (metadata[\"width\"] - offset[0]) // size\n",
    "                y_blocks = (metadata[\"height\"] - offset[1]) // size\n",
    "            block_size = x_blocks * y_blocks\n",
    "            raster_pass = reconstitute_raster(  # pylint: disable=too-many-function-args\n",
    "                blocks[previous : block_size + previous, :, :, :],\n",
    "                metadata[\"height\"],\n",
    "                metadata[\"width\"],\n",
    "                size,\n",
    "                offset,\n",
    "                border_patches,\n",
    "                border_patches_x,\n",
    "                border_patches_y,\n",
    "            )\n",
    "            if raster_pass.shape[1] > largest_x:\n",
    "                largest_x = raster_pass.shape[1]\n",
    "            if raster_pass.shape[0] > largest_y:\n",
    "                largest_y = raster_pass.shape[0]\n",
    "            previous += block_size\n",
    "            passes[index][\n",
    "                offset[1] : raster_pass.shape[0] + offset[1],\n",
    "                offset[0] : raster_pass.shape[1] + offset[0],\n",
    "                :,\n",
    "            ] = raster_pass\n",
    "            passes[index] = passes[index].filled(np.nan)\n",
    "        if merge_method == \"median\":\n",
    "            raster = np.nanmedian(passes, axis=0)\n",
    "        elif merge_method == \"mean\" or merge_method == \"average\":\n",
    "            raster = np.nanmean(passes, axis=0)\n",
    "        elif merge_method == \"min\" or merge_method == \"minumum\":\n",
    "            raster = np.nanmin(passes, axis=0)\n",
    "        elif merge_method == \"max\" or merge_method == \"maximum\":\n",
    "            raster = np.nanmax(passes, axis=0)\n",
    "        elif merge_method == \"mode\" or merge_method == \"majority\":\n",
    "            for index, _ in enumerate(passes):\n",
    "                passes[index] = np.rint(passes[index]).astype(int)\n",
    "            raster = np.apply_along_axis(\n",
    "                lambda x: np.bincount(x).argmax(), axis=0, arr=passes\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unable to parse merge_method: {merge_method}\")\n",
    "    else:\n",
    "        raster: np.ndarray = reconstitute_raster(\n",
    "            blocks,\n",
    "            metadata[\"height\"],\n",
    "            metadata[\"width\"],\n",
    "            size,\n",
    "            (0, 0),\n",
    "            border_patches,\n",
    "            border_patches_x,\n",
    "            border_patches_y,\n",
    "        )\n",
    "    if output_array:\n",
    "        return raster\n",
    "    return array_to_raster(raster, reference, out_path=out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_blockshape(shape: tuple, block_shape: tuple, offset: tuple) -> list:\n",
    "    \"\"\"Calculates the shape of the output array.\n",
    "    Args:\n",
    "        shape (tuple | list): The shape if the original raster. (1980, 1080, 3)\n",
    "        block_shape (tuple | list): The size of the blocks eg. (64, 64)\n",
    "        offset (tuple, list): An initial offset for the array eg. (32, 32)\n",
    "    Returns:\n",
    "        A list with the modified shape.\n",
    "    \"\"\"\n",
    "    type_check(shape, [tuple], \"shape\")\n",
    "    type_check(block_shape, [tuple], \"block_shape\")\n",
    "    type_check(offset, [tuple], \"offset\")\n",
    "\n",
    "    # import pdb\n",
    "\n",
    "    # pdb.set_trace()\n",
    "    assert len(offset) == 2, \"Offset has to be two dimensional.\"\n",
    "    assert len(shape) == 3, \"Shape has to be three dimensional.\"\n",
    "    assert len(block_shape) == 2, \"Shape of block has to be two dimensional.\"\n",
    "    base_shape = list(shape)\n",
    "    for index, value in enumerate(offset):\n",
    "        base_shape[index] = base_shape[index] - value\n",
    "    sizes = [base_shape[0] // block_shape[0], base_shape[1] // block_shape[1]]\n",
    "    return sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_blocks(\n",
    "    array: np.ndarray,\n",
    "    block_shape: tuple,\n",
    "    offset: tuple,\n",
    "    border_patches_x: bool = False,\n",
    "    border_patches_y: bool = False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Turns an array into a series of blocks. The array can be offset.\n",
    "    Args:\n",
    "        array (ndarray): The array to turn to blocks. (Channel last format: 1920x1080x3)\n",
    "        block_shape (tuple | list | ndarray): The size of the blocks eg. (64, 64)\n",
    "        offset (tuple, list, ndarray): An initial offset for the array eg. (32, 32)\n",
    "    Returns:\n",
    "        A modified view into the array.\n",
    "    \"\"\"\n",
    "    type_check(array, [np.ndarray], \"array\")\n",
    "    type_check(block_shape, [tuple], \"block_shape\")\n",
    "    type_check(offset, [tuple], \"offset\")\n",
    "    type_check(border_patches_x, [bool], \"border_patches_x\")\n",
    "    type_check(border_patches_y, [bool], \"border_patches_y\")\n",
    "    assert array.ndim == 3, \"Input raster must be three dimensional\"\n",
    "    arr = array[\n",
    "        offset[1] : int(\n",
    "            array.shape[0] - ((array.shape[0] - offset[1]) % block_shape[0])\n",
    "        ),\n",
    "        offset[0] : int(\n",
    "            array.shape[1] - ((array.shape[1] - offset[0]) % block_shape[1])\n",
    "        ),\n",
    "        :,\n",
    "    ]\n",
    "\n",
    "    # If border patches are needed. A new array must be created to hold the border values.\n",
    "    if border_patches_x or border_patches_y:\n",
    "        x_shape = arr.shape[1]\n",
    "        y_shape = arr.shape[0]\n",
    "        if border_patches_x:\n",
    "            x_shape = (\n",
    "                (array.shape[1] // block_shape[0]) * block_shape[0]\n",
    "            ) + block_shape[0]\n",
    "        if border_patches_y:\n",
    "            y_shape = (\n",
    "                (array.shape[0] // block_shape[1]) * block_shape[1]\n",
    "            ) + block_shape[1]\n",
    "\n",
    "        # Expand and copy the original array\n",
    "        arr_exp = np.empty((y_shape, x_shape, array.shape[2]), dtype=array.dtype)\n",
    "        arr_exp[0 : arr.shape[0], 0 : arr.shape[1], 0 : arr.shape[2]] = arr\n",
    "        if border_patches_x:\n",
    "            arr_exp[: array.shape[0], -block_shape[1] :, :] = array[\n",
    "                :, -block_shape[0] :, :\n",
    "            ]\n",
    "        if border_patches_y:\n",
    "            arr_exp[-block_shape[0] :, : array.shape[1], :] = array[\n",
    "                -block_shape[1] :, :, :\n",
    "            ]\n",
    "\n",
    "        # The bottom right corner will still have empty values if both a True\n",
    "        if border_patches_x and border_patches_y:\n",
    "            border_square = array[-block_shape[0] :, -block_shape[1] :, :]\n",
    "            arr_exp[-block_shape[0] :, -block_shape[1] :, :] = border_square\n",
    "        arr = arr_exp\n",
    "\n",
    "    # This only creates views into the array, so should still be fast.\n",
    "    reshaped = arr.reshape(\n",
    "        arr.shape[0] // block_shape[0],\n",
    "        block_shape[0],\n",
    "        arr.shape[1] // block_shape[1],\n",
    "        block_shape[1],\n",
    "        arr.shape[2],\n",
    "    )\n",
    "    swaped = reshaped.swapaxes(1, 2)\n",
    "    merge = swaped.reshape(-1, block_shape[0], block_shape[1], array.shape[2])\n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_extraction(\n",
    "    rasters: Union[list, str, gdal.Dataset],\n",
    "    arrays: Union[list, np.ndarray],\n",
    "    grid: Union[ogr.DataSource, str],\n",
    "    samples: int = 1000,  # if 0, all\n",
    "    grid_layer_index: int = 0,\n",
    "    verbose: int = 1,\n",
    ") -> bool:\n",
    "    \"\"\"Validates the output of the patch_extractor. Useful if you need peace of mind.\n",
    "    Set samples to 0 to tests everything.\n",
    "    Args:\n",
    "        rasters (list of rasters | path | raster): The raster(s) used.\n",
    "        arrays (list of arrays | ndarray): The arrays generated.\n",
    "        grid (vector | vector_path): The grid generated.\n",
    "    **kwargs:\n",
    "        samples (int): The amount of patches to randomly test. If 0 all patches will be\n",
    "        tested. This is a long process, so consider only testing everything if absolutely\n",
    "        necessary.\n",
    "        grid_layer_index (int): If the grid is part of a multi-layer vector, specify the\n",
    "        index of the grid.\n",
    "        verbose (int): If 1 will output messages on progress.\n",
    "    Returns:\n",
    "        True if the extraction is valid. Raises an error otherwise.\n",
    "    \"\"\"\n",
    "    type_check(rasters, [list, str, gdal.Dataset], \"rasters\")\n",
    "    type_check(arrays, [list, str, np.ndarray], \"arrays\")\n",
    "    type_check(grid, [list, str, ogr.DataSource], \"grid\")\n",
    "    type_check(samples, [int], \"samples\")\n",
    "    type_check(grid_layer_index, [int], \"clip_layer_index\")\n",
    "    type_check(verbose, [int], \"verbose\")\n",
    "    in_rasters = to_raster_list(rasters)\n",
    "    in_arrays = arrays\n",
    "    if verbose == 1:\n",
    "        print(\"Verifying integrity of output grid..\")\n",
    "\n",
    "    # grid_memory = open_vector(internal_vector_to_memory(grid))\n",
    "    grid_memory = open_vector(grid)\n",
    "    grid_metadata = internal_vector_to_metadata(grid)\n",
    "    grid_projection = grid_metadata[\"projection_osr\"]\n",
    "    if grid_layer_index > (grid_metadata[\"layer_count\"] - 1):\n",
    "        raise ValueError(f\"Requested non-existing layer index: {grid_layer_index}\")\n",
    "    grid_layer = grid_memory.GetLayer(grid_layer_index)\n",
    "\n",
    "    # Select sample fids\n",
    "    feature_count = grid_metadata[\"layers\"][grid_layer_index][\"feature_count\"]\n",
    "    test_samples = samples if samples > 0 else feature_count\n",
    "    max_test = min(test_samples, feature_count) - 1\n",
    "    test_fids = np.array(\n",
    "        random.sample(range(0, feature_count), max_test), dtype=\"uint64\"\n",
    "    )\n",
    "    mem_driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "    for index, raster in enumerate(in_rasters):\n",
    "        test_rast = open_raster(raster)\n",
    "        test_array = in_arrays[index]\n",
    "        if isinstance(test_array, str):\n",
    "            if not os.path.exists(test_array):\n",
    "                raise ValueError(f\"Numpy array does not exist: {test_array}\")\n",
    "            try:\n",
    "                test_array = np.load(in_arrays[index])\n",
    "            except:\n",
    "                raise Exception(\n",
    "                    f\"Attempted to read numpy raster from: {in_arrays[index]}\"\n",
    "                )\n",
    "        base = os.path.basename(raster)\n",
    "        basename = os.path.splitext(base)[0]\n",
    "        if verbose == 1:\n",
    "            print(f\"Testing: {basename}\")\n",
    "        tested = 0\n",
    "        for test in test_fids:\n",
    "            feature = grid_layer.GetFeature(test)\n",
    "            if feature is None:\n",
    "                raise Exception(f\"Feature not found: {test}\")\n",
    "            test_ds_path = f\"/vsimem/test_mem_grid_{uuid4().int}.gpkg\"\n",
    "            test_ds = mem_driver.CreateDataSource(test_ds_path)\n",
    "            test_ds_lyr = test_ds.CreateLayer(\n",
    "                \"test_mem_grid_layer\", geom_type=ogr.wkbPolygon, srs=grid_projection\n",
    "            )\n",
    "            test_ds_lyr.CreateFeature(feature.Clone())\n",
    "            test_ds.SyncToDisk()\n",
    "            clipped = internal_clip_raster(\n",
    "                test_rast,\n",
    "                test_ds_path,\n",
    "                adjust_bbox=False,\n",
    "                crop_to_geom=True,\n",
    "                all_touch=False,\n",
    "            )\n",
    "            if clipped is None:\n",
    "                raise Exception(\"Error while clipping raster. Likely a bad extraction.\")\n",
    "            ref_image = raster_to_array(clipped, filled=True)\n",
    "            image_block = test_array[test]\n",
    "            if not np.array_equal(ref_image, image_block):\n",
    "                # from matplotlib import pyplot as plt; plt.imshow(ref_image[:,:,0]); plt.show()\n",
    "                raise Exception(f\"Image {basename} and grid cell did not match..\")\n",
    "            if verbose == 1:\n",
    "                progress(tested, len(test_fids) - 1, \"Verifying..\")\n",
    "            tested += 1\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Initial clip to extent of clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(\n",
    "    raster: Union[List[Union[str, gdal.Dataset]], str, gdal.Dataset],\n",
    "    out_dir: Optional[str] = None,\n",
    "    prefix: str = \"\",\n",
    "    postfix: str = \"_patches\",\n",
    "    size: int = 32,\n",
    "    offsets: Union[list, None] = [],\n",
    "    generate_border_patches: bool = True,\n",
    "    generate_zero_offset: bool = True,\n",
    "    generate_grid_geom: bool = True,\n",
    "    clip_geom: Optional[Union[str, ogr.DataSource, gdal.Dataset]] = None,\n",
    "    clip_layer_index: int = 0,\n",
    "    verify_output=True,\n",
    "    verification_samples=100,\n",
    "    overwrite=True,\n",
    "    epsilon: float = 1e-9,\n",
    "    verbose: int = 1,\n",
    ") -> tuple:\n",
    "    \"\"\"Extracts square tiles from a raster.\n",
    "    Args:\n",
    "        raster (list of rasters | path | raster): The raster(s) to convert.\n",
    "    **kwargs:\n",
    "        out_dir (path | none): Folder to save output. If None, in-memory\n",
    "        arrays and geometries are outputted.\n",
    "        prefix (str): A prefix for all outputs.\n",
    "        postfix (str): A postfix for all outputs.\n",
    "        size (int): The size of the tiles in pixels.\n",
    "        offsets (list of tuples): List of offsets to extract. Example:\n",
    "        offsets=[(16, 16), (16, 0), (0, 16)]. Will offset the initial raster\n",
    "        and extract from there.\n",
    "        generate_border_patches (bool): The tiles often do not align with the\n",
    "        rasters which means borders are trimmed somewhat. If generate_border_patches\n",
    "        is True, an additional tile is added where needed.\n",
    "        generate_zero_offset (bool): if True, an offset is inserted at (0, 0)\n",
    "        if none is present.\n",
    "        generate_grid_geom (bool): Output a geopackage with the grid of tiles.\n",
    "        clip_geom (str, raster, vector): Clip the output to the\n",
    "        intersections with a geometry. Useful if a lot of the target\n",
    "        area is water or similar.\n",
    "        epsilon (float): How much for buffer the arange array function. This\n",
    "        should usually just be left alone.\n",
    "        verbose (int): If 1 will output messages on progress.\n",
    "    Returns:\n",
    "        A tuple with paths to the generated items. (numpy_array, grid_geom)\n",
    "    \"\"\"\n",
    "    type_check(raster, [str, list, gdal.Dataset], \"raster\")\n",
    "    type_check(out_dir, [str], \"out_dir\", allow_none=True)\n",
    "    type_check(prefix, [str], \"prefix\")\n",
    "    type_check(postfix, [str], \"postfix\")\n",
    "    type_check(size, [int], \"size\")\n",
    "    type_check(offsets, [list], \"offsets\", allow_none=True)\n",
    "    type_check(generate_grid_geom, [bool], \"generate_grid_geom\")\n",
    "    type_check(\n",
    "        clip_geom,\n",
    "        [str, ogr.DataSource, gdal.Dataset],\n",
    "        \"clip_layer_index\",\n",
    "        allow_none=True,\n",
    "    )\n",
    "    type_check(clip_layer_index, [int], \"clip_layer_index\")\n",
    "    type_check(overwrite, [bool], \"overwrite\")\n",
    "    type_check(epsilon, [float], \"epsilon\")\n",
    "    type_check(verbose, [int], \"verbose\")\n",
    "    in_rasters = to_raster_list(raster)\n",
    "    if out_dir is not None and not os.path.isdir(out_dir):\n",
    "        raise ValueError(f\"Output directory does not exists: {out_dir}\")\n",
    "    if not rasters_are_aligned(in_rasters):\n",
    "        raise ValueError(\n",
    "            \"Input rasters must be aligned. Please use the align function.\"\n",
    "        )\n",
    "    output_geom = None\n",
    "    metadata = internal_raster_to_metadata(in_rasters[0])\n",
    "    if verbose == 1:\n",
    "        print(\"Generating blocks..\")\n",
    "\n",
    "    # internal offset array. Avoid manipulating the og array.\n",
    "    if offsets is None:\n",
    "        offsets = []\n",
    "    in_offsets = []\n",
    "    if generate_zero_offset and (0, 0) not in offsets:\n",
    "        in_offsets.append((0, 0))\n",
    "    for offset in offsets:\n",
    "        if offset != (0, 0):\n",
    "            if not isinstance(offset, (list, tuple)) or len(offset) != 2:\n",
    "                raise ValueError(\n",
    "                    f\"offset must be a list or tuple of two integers. Recieved: {offset}\"\n",
    "                )\n",
    "            in_offsets.append((offset[0], offset[1]))\n",
    "    border_patches_needed_x = True\n",
    "    border_patches_needed_y = True\n",
    "    if clip_geom is not None:\n",
    "        border_patches_needed_x = False\n",
    "        border_patches_needed_y = False\n",
    "    shapes = []\n",
    "    for offset in in_offsets:\n",
    "        block_shape = shape_to_blockshape(metadata[\"shape\"], (size, size), offset)\n",
    "        if block_shape[0] * size == metadata[\"width\"]:\n",
    "            border_patches_needed_x = False\n",
    "        if block_shape[1] * size == metadata[\"height\"]:\n",
    "            border_patches_needed_y = False\n",
    "        shapes.append(block_shape)\n",
    "    if generate_border_patches:\n",
    "        cut_x = (metadata[\"width\"] - in_offsets[0][0]) - (shapes[0][0] * size)\n",
    "        cut_y = (metadata[\"height\"] - in_offsets[0][1]) - (shapes[0][1] * size)\n",
    "        if border_patches_needed_x and cut_x > 0:\n",
    "            shapes[0][0] += 1\n",
    "        if border_patches_needed_y and cut_y > 0:\n",
    "            shapes[0][1] += 1\n",
    "\n",
    "    # calculate the offsets\n",
    "    all_rows = 0\n",
    "    offset_rows = []\n",
    "    for i in range(len(shapes)):\n",
    "        row = 0\n",
    "        for j in range(len(shapes[i])):\n",
    "            if j == 0:\n",
    "                row = int(shapes[i][j])\n",
    "            else:\n",
    "                row *= int(shapes[i][j])\n",
    "        offset_rows.append(row)\n",
    "        all_rows += row\n",
    "    offset_rows_cumsum = np.cumsum(offset_rows)\n",
    "    if generate_grid_geom is True or clip_geom is not None:\n",
    "        if verbose == 1:\n",
    "            print(\"Calculating grid cells..\")\n",
    "        mask = np.arange(all_rows, dtype=\"uint64\")\n",
    "        ulx, uly, _lrx, _lry = metadata[\"extent\"]\n",
    "        pixel_width = abs(metadata[\"pixel_width\"])\n",
    "        pixel_height = abs(metadata[\"pixel_height\"])\n",
    "        xres = pixel_width * size\n",
    "        yres = pixel_height * size\n",
    "        dx = xres / 2\n",
    "        dy = yres / 2\n",
    "\n",
    "        # Ready clip geom outside of loop.\n",
    "        if clip_geom is not None:\n",
    "            clip_ref = open_vector(\n",
    "                internal_reproject_vector(clip_geom, metadata[\"projection_osr\"])\n",
    "            )\n",
    "            clip_layer = clip_ref.GetLayerByIndex(clip_layer_index)\n",
    "            meta_clip = internal_vector_to_metadata(clip_ref)\n",
    "            # geom_clip = meta_clip[\"layers\"][clip_layer_index][\"column_geom\"]\n",
    "            clip_extent = meta_clip[\"extent_ogr\"]\n",
    "            # clip_adjust = [\n",
    "            #     clip_extent[0] - clip_extent[0] % xres,  # x_min\n",
    "            #     (clip_extent[1] - clip_extent[1] % xres) + xres,  # x_max\n",
    "            #     clip_extent[2] - clip_extent[2] % yres,  # y_min\n",
    "            #     (clip_extent[3] - clip_extent[3] % yres) + yres,  # y_max\n",
    "            # ]\n",
    "        coord_grid = np.empty((all_rows, 2), dtype=\"float64\")\n",
    "\n",
    "        # tiled_extent = [None, None, None, None]\n",
    "        row_count = 0\n",
    "        for idx in range(len(in_offsets)):\n",
    "            x_offset = in_offsets[idx][0]\n",
    "            y_offset = in_offsets[idx][1]\n",
    "            x_step = shapes[idx][0]\n",
    "            y_step = shapes[idx][1]\n",
    "            x_min = (ulx + dx) + (x_offset * pixel_width)\n",
    "            x_max = x_min + (x_step * xres)\n",
    "            y_max = (uly - dy) - (y_offset * pixel_height)\n",
    "            y_min = y_max - (y_step * yres)\n",
    "\n",
    "            # if clip_geom is not None:\n",
    "            #     if clip_adjust[0] > x_min:\n",
    "            #         x_min = clip_adjust[0] + (x_offset * pixel_width)\n",
    "            #     if clip_adjust[1] < x_max:\n",
    "            #         x_max = clip_adjust[1] + (x_offset * pixel_width)\n",
    "            #     if clip_adjust[2] > y_min:\n",
    "            #         y_min = clip_adjust[2] - (y_offset * pixel_height)\n",
    "            #     if clip_adjust[3] < y_max:\n",
    "            #         y_max = clip_adjust[3] - (y_offset * pixel_height)\n",
    "\n",
    "            # if idx == 0:\n",
    "            #     tiled_extent[0] = x_min\n",
    "            #     tiled_extent[1] = x_max\n",
    "            #     tiled_extent[2] = y_min\n",
    "            #     tiled_extent[3] = y_max\n",
    "            # else:\n",
    "            #     if x_min < tiled_extent[0]:\n",
    "            #         tiled_extent[0] = x_min\n",
    "            #     if x_max > tiled_extent[1]:\n",
    "            #         tiled_extent[1] = x_max\n",
    "            #     if y_min < tiled_extent[2]:\n",
    "            #         tiled_extent[2] = y_min\n",
    "            #     if y_max > tiled_extent[3]:\n",
    "            #         tiled_extent[3] = y_max\n",
    "\n",
    "            # y is flipped so: xmin --> xmax, ymax -- ymin to keep same order as numpy array\n",
    "            x_patches = round((x_max - x_min) / xres)\n",
    "            y_patches = round((y_max - y_min) / yres)\n",
    "            xr = np.arange(x_min, x_max, xres)[0:x_step]\n",
    "            if xr.shape[0] < x_patches:\n",
    "                xr = np.arange(x_min, x_max + epsilon, xres)[0:x_step]\n",
    "            elif xr.shape[0] > x_patches:\n",
    "                xr = np.arange(x_min, x_max - epsilon, xres)[0:x_step]\n",
    "            yr = np.arange(y_max, y_min + epsilon, -yres)[0:y_step]\n",
    "            if yr.shape[0] < y_patches:\n",
    "                yr = np.arange(y_max, y_min - epsilon, -yres)[0:y_step]\n",
    "            elif yr.shape[0] > y_patches:\n",
    "                yr = np.arange(y_max, y_min + epsilon, -yres)[0:y_step]\n",
    "            if generate_border_patches and idx == 0:\n",
    "                if border_patches_needed_x:\n",
    "                    xr[-1] = xr[-1] - ((xr[-1] + dx) - metadata[\"extent_dict\"][\"right\"])\n",
    "                if border_patches_needed_y:\n",
    "                    yr[-1] = yr[-1] - (\n",
    "                        (yr[-1] - dy) - metadata[\"extent_dict\"][\"bottom\"]\n",
    "                    )\n",
    "            oxx, oyy = np.meshgrid(xr, yr)\n",
    "            oxr = oxx.ravel()\n",
    "            oyr = oyy.ravel()\n",
    "            offset_length = oxr.shape[0]\n",
    "            coord_grid[row_count : row_count + offset_length, 0] = oxr\n",
    "            coord_grid[row_count : row_count + offset_length, 1] = oyr\n",
    "            row_count += offset_length\n",
    "            offset_rows_cumsum[idx] = offset_length\n",
    "        offset_rows_cumsum = np.cumsum(offset_rows_cumsum)\n",
    "        coord_grid = coord_grid[:row_count]\n",
    "\n",
    "        # Output geometry\n",
    "        driver = ogr.GetDriverByName(\"GPKG\")\n",
    "        patches_path = f\"/vsimem/patches_{uuid4().int}.gpkg\"\n",
    "        patches_ds = driver.CreateDataSource(patches_path)\n",
    "        patches_layer = patches_ds.CreateLayer(\n",
    "            \"patches_all\", geom_type=ogr.wkbPolygon, srs=metadata[\"projection_osr\"]\n",
    "        )\n",
    "        patches_fdefn = patches_layer.GetLayerDefn()\n",
    "        og_fid = \"og_fid\"\n",
    "        field_defn = ogr.FieldDefn(og_fid, ogr.OFTInteger)\n",
    "        patches_layer.CreateField(field_defn)\n",
    "        if clip_geom is not None:\n",
    "            clip_feature_count = meta_clip[\"layers\"][clip_layer_index][\"feature_count\"]\n",
    "            spatial_index = rtree.index.Index(interleaved=False)\n",
    "            for _ in range(clip_feature_count):\n",
    "                clip_feature = clip_layer.GetNextFeature()\n",
    "                clip_fid = clip_feature.GetFID()\n",
    "                clip_feature_geom = clip_feature.GetGeometryRef()\n",
    "                xmin, xmax, ymin, ymax = clip_feature_geom.GetEnvelope()\n",
    "                spatial_index.insert(clip_fid, (xmin, xmax, ymin, ymax))\n",
    "        fids = 0\n",
    "        mask = []\n",
    "        for tile_id in range(coord_grid.shape[0]):\n",
    "            x, y = coord_grid[tile_id]\n",
    "            if verbose == 1:\n",
    "                progress(tile_id, coord_grid.shape[0], \"Patch generation\")\n",
    "            x_min = x - dx\n",
    "            x_max = x + dx\n",
    "            y_min = y - dx\n",
    "            y_max = y + dx\n",
    "            tile_intersects = True\n",
    "            grid_geom = None\n",
    "            poly_wkt = None\n",
    "            if clip_geom is not None:\n",
    "                tile_intersects = False\n",
    "                if not ogr_bbox_intersects([x_min, x_max, y_min, y_max], clip_extent):\n",
    "                    continue\n",
    "                intersections = list(\n",
    "                    spatial_index.intersection((x_min, x_max, y_min, y_max))\n",
    "                )\n",
    "                if len(intersections) == 0:\n",
    "                    continue\n",
    "                poly_wkt = f\"POLYGON (({x_min} {y_max}, {x_max} {y_max}, {x_max} {y_min}, {x_min} {y_min}, {x_min} {y_max}))\"\n",
    "                grid_geom = ogr.CreateGeometryFromWkt(poly_wkt)\n",
    "                for fid1 in intersections:\n",
    "                    clip_feature = clip_layer.GetFeature(fid1)\n",
    "                    clip_geom = clip_feature.GetGeometryRef()\n",
    "                    if grid_geom.Intersects(clip_geom):\n",
    "                        tile_intersects = True\n",
    "                        continue\n",
    "            if tile_intersects:\n",
    "                ft = ogr.Feature(patches_fdefn)\n",
    "                if grid_geom is None:\n",
    "                    poly_wkt = f\"POLYGON (({x_min} {y_max}, {x_max} {y_max}, {x_max} {y_min}, {x_min} {y_min}, {x_min} {y_max}))\"\n",
    "                    grid_geom = ogr.CreateGeometryFromWkt(poly_wkt)\n",
    "                ft_geom = ogr.CreateGeometryFromWkt(poly_wkt)\n",
    "                ft.SetGeometry(ft_geom)\n",
    "                ft.SetField(og_fid, int(fids))\n",
    "                ft.SetFID(int(fids))\n",
    "                patches_layer.CreateFeature(ft)\n",
    "                ft = None\n",
    "                mask.append(tile_id)\n",
    "                fids += 1\n",
    "        if verbose == 1:\n",
    "            progress(coord_grid.shape[0], coord_grid.shape[0], \"Patch generation\")\n",
    "        mask = np.array(mask, dtype=int)\n",
    "        if generate_grid_geom is True:\n",
    "            if out_dir is None:\n",
    "                output_geom = patches_ds\n",
    "            else:\n",
    "                raster_basename = metadata[\"basename\"]\n",
    "                geom_name = f\"{prefix}{raster_basename}_geom_{str(size)}{postfix}.gpkg\"\n",
    "                output_geom = os.path.join(out_dir, geom_name)\n",
    "                overwrite_required(output_geom, overwrite)\n",
    "                remove_if_overwrite(output_geom, overwrite)\n",
    "                if verbose == 1:\n",
    "                    print(\"Writing output geometry..\")\n",
    "                internal_vector_to_disk(patches_ds, output_geom, overwrite=overwrite)\n",
    "    if verbose == 1:\n",
    "        print(\"Writing numpy array..\")\n",
    "    output_blocks = []\n",
    "    for raster in in_rasters:\n",
    "        base = None\n",
    "        basename = None\n",
    "        output_block = None\n",
    "        if out_dir is not None:\n",
    "            base = os.path.basename(raster)\n",
    "            basename = os.path.splitext(base)[0]\n",
    "            output_block = os.path.join(out_dir + f\"{prefix}{basename}{postfix}.npy\")\n",
    "        metadata = internal_raster_to_metadata(raster)\n",
    "        if generate_grid_geom is True or clip_geom is not None:\n",
    "            output_shape = (row_count, size, size, metadata[\"band_count\"])\n",
    "        else:\n",
    "            output_shape = (all_rows, size, size, metadata[\"band_count\"])\n",
    "        input_datatype = metadata[\"datatype\"]\n",
    "        output_array = np.empty(output_shape, dtype=input_datatype)\n",
    "\n",
    "        # if clip_geom is not None:\n",
    "        #     ref = raster_to_array(raster, filled=True, extent=tiled_extent)\n",
    "        # else:\n",
    "        ref = raster_to_array(raster, filled=True)\n",
    "        for k, offset in enumerate(in_offsets):\n",
    "            start = 0\n",
    "            if k > 0:\n",
    "                start = offset_rows_cumsum[k - 1]\n",
    "            blocks = None\n",
    "            if (\n",
    "                k == 0\n",
    "                and generate_border_patches\n",
    "                and (border_patches_needed_x or border_patches_needed_y)\n",
    "            ):\n",
    "                blocks = array_to_blocks(\n",
    "                    ref,\n",
    "                    (size, size),\n",
    "                    offset,\n",
    "                    border_patches_needed_x,\n",
    "                    border_patches_needed_y,\n",
    "                )\n",
    "            else:\n",
    "                blocks = array_to_blocks(ref, (size, size), offset)\n",
    "            output_array[start : offset_rows_cumsum[k]] = blocks\n",
    "        if generate_grid_geom is False and clip_geom is None:\n",
    "            if out_dir is None:\n",
    "                output_blocks.append(output_array)\n",
    "            else:\n",
    "                output_blocks.append(output_block)\n",
    "                np.save(output_block, output_array)\n",
    "        else:\n",
    "            if out_dir is None:\n",
    "                output_blocks.append(output_array[mask])\n",
    "            else:\n",
    "                output_blocks.append(output_block)\n",
    "                np.save(output_block, output_array[mask])\n",
    "    if verify_output and generate_grid_geom:\n",
    "        test_extraction(\n",
    "            in_rasters,\n",
    "            output_blocks,\n",
    "            output_geom,\n",
    "            samples=verification_samples,\n",
    "            grid_layer_index=0,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "    if len(output_blocks) == 1:\n",
    "        output_blocks = output_blocks[0]\n",
    "    return (output_blocks, output_geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_raster(\n",
    "    raster: Union[List[Union[str, gdal.Dataset]], str, gdal.Dataset],\n",
    "    model: str,\n",
    "    out_path: Optional[str] = None,\n",
    "    offsets: Union[List[Tuple[int, int]], List[List[Tuple[int, int]]]] = [],\n",
    "    region: Optional[Union[str, ogr.DataSource]] = None,\n",
    "    device: str = \"gpu\",\n",
    "    merge_method: str = \"median\",\n",
    "    mirror: bool = False,\n",
    "    rotate: bool = False,\n",
    "    custom_objects: Dict[str, Any] = {\"Mish\": Mish, \"mish\": mish, \"tpe\": tpe},\n",
    "    target_raster: Optional[str] = None,\n",
    "    output_size=128,\n",
    "    dtype: str = \"same\",\n",
    "    batch_size: int = 16,\n",
    "    overwrite: bool = True,\n",
    "    creation_options: List[str] = [],\n",
    "    verbose: int = 1,\n",
    ") -> str:\n",
    "    \"\"\"Runs a raster or list of rasters through a deep learning network (Tensorflow).\n",
    "        Supports tiling and reconstituting the output. Offsets are allowed and will be\n",
    "        bleneded with the merge_method. If the output is a different resolution\n",
    "        than the input. The output will automatically be scaled to match.\n",
    "    Args:\n",
    "        raster (list | path | raster): The raster(s) to convert.\n",
    "        model (path): A path to the tensorflow .h5 model.\n",
    "    **kwargs:\n",
    "        out_path (str | None): Where to save the reconstituted raster. If None\n",
    "        are memory raster is returned.\n",
    "        offsets (tuple, list, ndarray): The offsets used in the original. A (0 ,0)\n",
    "        offset is assumed.\n",
    "        border_patches (bool): Do the blocks contain border patches?\n",
    "        device (str): Either CPU or GPU to use with tensorflow.\n",
    "        merge_method (str): How to handle overlapping pixels. Options are:\n",
    "        median, average, mode, min, max\n",
    "        mirror (bool): Mirror the raster and do predictions as well.\n",
    "        rotate (bool): rotate the raster and do predictions as well.\n",
    "        dtype (str | None): The dtype of the output. If None: Float32, \"save\"\n",
    "        is the same as the input raster. Otherwise overwrite dtype.\n",
    "        overwrite (bool): Overwrite output files if they exists.\n",
    "        creation_options: Extra creation options for the output raster.\n",
    "        verbose (int): If 1 will output messages on progress.\n",
    "    Returns:\n",
    "        A predicted raster.\n",
    "    \"\"\"\n",
    "    type_check(raster, [list, str, gdal.Dataset], \"raster\")\n",
    "    type_check(model, [str], \"model\")\n",
    "    type_check(out_path, [str], \"out_path\", allow_none=True)\n",
    "    type_check(offsets, [list], \"offsets\")\n",
    "    type_check(region, [str, ogr.DataSource], allow_none=True)\n",
    "    type_check(device, [str], \"device\")\n",
    "    type_check(merge_method, [str], \"merge_method\")\n",
    "    type_check(mirror, [bool], \"mirror\")\n",
    "    type_check(rotate, [bool], \"rotate\")\n",
    "    type_check(custom_objects, [dict], \"custom_objects\")\n",
    "    type_check(dtype, [str], \"dtype\", allow_none=True)\n",
    "    type_check(batch_size, [int], \"batch_size\")\n",
    "    type_check(overwrite, [bool], \"overwrite\")\n",
    "    type_check(creation_options, [list], \"creation_options\")\n",
    "    type_check(verbose, [int], \"verbose\")\n",
    "    if mirror or rotate:\n",
    "        raise Exception(\"Mirror and rotate currently disabled.\")\n",
    "    import tensorflow as tf\n",
    "    os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "    if verbose == 1:\n",
    "        print(\"Loading model.\")\n",
    "    if isinstance(model, str):\n",
    "        model_loaded = tf.keras.models.load_model(model, custom_objects=custom_objects)\n",
    "    else:\n",
    "        model_loaded = model\n",
    "    multi_input = False\n",
    "    if isinstance(model_loaded.input, list) and len(model_loaded.input) > 1:\n",
    "        if not isinstance(raster, list):\n",
    "            raise TypeError(\"Multi input model must have a list as input.\")\n",
    "        if len(offsets) > 0:\n",
    "            for offset in offsets:\n",
    "                if not isinstance(offset, list):\n",
    "                    raise TypeError(\n",
    "                        \"Offsets must be a list of tuples, same length as inputs.\"\n",
    "                    )\n",
    "                for _offset in offset:\n",
    "                    if not isinstance(_offset, tuple):\n",
    "                        raise TypeError(\"Offset must be a tuple\")\n",
    "                    if len(_offset) != 2:\n",
    "                        raise ValueError(\"Offset must be length 2.\")\n",
    "            if len(model_loaded.input) != len(offsets):\n",
    "                raise ValueError(\"Length of offsets must equal model inputs.\")\n",
    "        multi_input = True\n",
    "    model_inputs = (\n",
    "        model_loaded.input\n",
    "        if isinstance(model_loaded.input, list)\n",
    "        else [model_loaded.input]\n",
    "    )\n",
    "    shape_output = tuple(model_loaded.output.shape)\n",
    "    if shape_output[1] == None or shape_output[2] == None or shape_output[3] == None:\n",
    "        print(f\"Unable to find output size, using the supplied variable: {output_size}\")\n",
    "        shape_output = (None, output_size, output_size, 1)\n",
    "    dst_tile_size = shape_output[1]\n",
    "    prediction_arr = []\n",
    "    readied_inputs = []\n",
    "    pixel_factor = 1.0\n",
    "    for index, model_input in enumerate(model_inputs):\n",
    "        if verbose == 1:\n",
    "            print(f\"Readying input: {index}\")\n",
    "        shape_input = tuple(model_input.shape)\n",
    "        if len(shape_input) != 4 or len(shape_output) != 4:\n",
    "            raise ValueError(f\"Model input not 4d: {shape_input} - {shape_output}\")\n",
    "        if shape_input[1] != shape_input[2] or shape_output[1] != shape_output[2]:\n",
    "            raise ValueError(\"Model only takes square images.\")\n",
    "        src_tile_size = shape_input[1]\n",
    "        pixel_factor = src_tile_size / dst_tile_size\n",
    "        scale_factor = dst_tile_size / src_tile_size\n",
    "        dst_offsets = []\n",
    "        in_offsets: List[Tuple[Number, Number]] = []\n",
    "        if multi_input:\n",
    "            if len(offsets) > 0:\n",
    "                in_offsets = offsets[index]\n",
    "        else:\n",
    "            in_offsets = offsets\n",
    "        for offset in in_offsets:\n",
    "            if not isinstance(offset, tuple):\n",
    "                raise ValueError(\n",
    "                    f\"Offset must be a tuple of two ints. Recieved: {offset}\"\n",
    "                )\n",
    "            if len(offset) != 2:\n",
    "                raise ValueError(\"Offsets must have two values. Both integers.\")\n",
    "            dst_offsets.append(\n",
    "                (\n",
    "                    round(offset[0] * scale_factor),\n",
    "                    round(offset[1] * scale_factor),\n",
    "                )\n",
    "            )\n",
    "        use_raster = raster[index] if isinstance(raster, list) else raster\n",
    "        if region is not None:\n",
    "            use_raster = clip_raster(\n",
    "                use_raster, region, adjust_bbox=False, all_touch=False\n",
    "            )\n",
    "\n",
    "        # # check raster size here\n",
    "        # use_raster_meta = internal_raster_to_metadata(use_raster)\n",
    "        # import pdb\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        blocks, _ = extract_patches(\n",
    "            use_raster,\n",
    "            size=src_tile_size,\n",
    "            offsets=in_offsets,\n",
    "            generate_border_patches=True,\n",
    "            generate_grid_geom=False,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        readied_inputs.append(blocks)\n",
    "    first_len = None\n",
    "    for index, readied in enumerate(readied_inputs):\n",
    "        if index == 0:\n",
    "            first_len = readied.shape[0]\n",
    "        else:\n",
    "            if readied.shape[0] != first_len:\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "                raise ValueError(\n",
    "                    \"Length of inputs do not match. Have you set the offsets in the correct order?\"\n",
    "                )\n",
    "    if verbose == 1:\n",
    "        print(\"Predicting raster.\")\n",
    "    start = 0\n",
    "    end = readied_inputs[0].shape[0]\n",
    "    predictions = np.empty(\n",
    "        (end, dst_tile_size, dst_tile_size, shape_output[3]), dtype=\"float32\"\n",
    "    )\n",
    "    if multi_input is False:\n",
    "        if device == \"cpu\":\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                while start < end:\n",
    "                    predictions[\n",
    "                        start : start + batch_size\n",
    "                    ] = model_loaded.predict_on_batch(\n",
    "                        readied_inputs[0][start : start + batch_size]\n",
    "                    )\n",
    "                    start += batch_size\n",
    "                    progress(start, end - 1, \"Predicting\")\n",
    "        else:\n",
    "            while start < end:\n",
    "                predictions[start : start + batch_size] = model_loaded.predict_on_batch(\n",
    "                    readied_inputs[0][start : start + batch_size]\n",
    "                )\n",
    "                start += batch_size\n",
    "                progress(start, end - 1, \"Predicting\")\n",
    "    else:\n",
    "        if device == \"cpu\":\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                while start < end:\n",
    "                    batch = []\n",
    "                    for i in range(len(readied_inputs)):\n",
    "                        batch.append(readied_inputs[i][start : start + batch_size])\n",
    "                    predictions[\n",
    "                        start : start + batch_size\n",
    "                    ] = model_loaded.predict_on_batch(batch)\n",
    "                    start += batch_size\n",
    "                    progress(start, end - 1, \"Predicting\")\n",
    "        else:\n",
    "            while start < end:\n",
    "                batch = []\n",
    "                for i in range(len(readied_inputs)):\n",
    "                    batch.append(readied_inputs[i][start : start + batch_size])\n",
    "                predictions[start : start + batch_size] = model_loaded.predict_on_batch(\n",
    "                    batch\n",
    "                )\n",
    "                start += batch_size\n",
    "                progress(start, end - 1, \"Predicting\")\n",
    "    print(\"\")\n",
    "    print(\"Reconstituting Raster.\")\n",
    "    rast_meta = None\n",
    "    target_size = None\n",
    "    resampled = None\n",
    "    if target_raster is not None:\n",
    "        resampled = target_raster\n",
    "    elif isinstance(raster, list):\n",
    "        rast_meta = internal_raster_to_metadata(raster[-1])\n",
    "        target_size = (\n",
    "            rast_meta[\"pixel_width\"] * pixel_factor,\n",
    "            rast_meta[\"pixel_height\"] * pixel_factor,\n",
    "        )\n",
    "        resampled = internal_resample_raster(\n",
    "            raster[-1], target_size=target_size, dtype=\"float32\"\n",
    "        )\n",
    "    else:\n",
    "        rast_meta = internal_raster_to_metadata(raster)\n",
    "        target_size = (\n",
    "            rast_meta[\"pixel_width\"] * pixel_factor,\n",
    "            rast_meta[\"pixel_height\"] * pixel_factor,\n",
    "        )\n",
    "        resampled = internal_resample_raster(\n",
    "            raster, target_size=target_size, dtype=\"float32\"\n",
    "        )\n",
    "    if region is not None:\n",
    "        resampled = clip_raster(resampled, region)\n",
    "    prediction_arr.append(\n",
    "        blocks_to_raster(\n",
    "            predictions,\n",
    "            resampled,\n",
    "            border_patches=True,\n",
    "            offsets=dst_offsets,\n",
    "            merge_method=merge_method,\n",
    "            output_array=True,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "    )\n",
    "    if verbose == 1:\n",
    "        print(\"Merging rasters.\")\n",
    "    if merge_method == \"median\":\n",
    "        predicted = np.median(prediction_arr, axis=0)\n",
    "    elif merge_method == \"mean\" or merge_method == \"average\":\n",
    "        predicted = np.mean(prediction_arr, axis=0)\n",
    "    elif merge_method == \"min\" or merge_method == \"minumum\":\n",
    "        predicted = np.min(prediction_arr, axis=0)\n",
    "    elif merge_method == \"max\" or merge_method == \"maximum\":\n",
    "        predicted = np.max(prediction_arr, axis=0)\n",
    "    elif merge_method == \"mode\" or merge_method == \"majority\":\n",
    "        for index, _ in enumerate(prediction_arr):\n",
    "            prediction_arr[index] = np.rint(prediction_arr[index]).astype(int)\n",
    "        predicted = np.apply_along_axis(\n",
    "            lambda x: np.bincount(x).argmax(), axis=0, arr=prediction_arr\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unable to parse merge_method: {merge_method}\")\n",
    "    if dtype == \"same\" or dtype == None:\n",
    "        predicted = array_to_raster(\n",
    "            predicted.astype(rast_meta[\"datatype\"]),\n",
    "            reference=resampled,\n",
    "        )\n",
    "    else:\n",
    "        predicted = array_to_raster(\n",
    "            predicted.astype(dtype),\n",
    "            reference=resampled,\n",
    "        )\n",
    "    if out_path is None:\n",
    "        return predicted\n",
    "    else:\n",
    "        return internal_raster_to_disk(\n",
    "            predicted,\n",
    "            out_path=out_path,\n",
    "            overwrite=overwrite,\n",
    "            creation_options=creation_options,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
